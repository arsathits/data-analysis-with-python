{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb5e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d34f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f63d1de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+----------+---------+\n",
      "| id|   name|department|salary|       dob|     city|\n",
      "+---+-------+----------+------+----------+---------+\n",
      "|  1|  Alice|        HR|  5000|1990-01-01|Bangalore|\n",
      "|  2|    Bob|   Finance|  6000|1985-07-23|   Mumbai|\n",
      "|  3|Charlie|        IT|  7000|1992-03-15|    Delhi|\n",
      "|  4|  David|        IT|  6500|1993-11-30|  Chennai|\n",
      "|  5|    Eva|        HR|  4800|1991-09-17|     Pune|\n",
      "+---+-------+----------+------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"dob\", StringType(), True),  # Could also be DateType\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.option(\"header\", True).schema(schema).csv(r\"D:\\MyPythonProject\\spark_datasets\\employees.csv\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37e0d316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+-------------+-------------+\n",
      "| id|   name|department|salary|date_of_birth|annual_salary|\n",
      "+---+-------+----------+------+-------------+-------------+\n",
      "|  1|  Alice|        HR|  5000|   1990-01-01|        60000|\n",
      "|  2|    Bob|   Finance|  6000|   1985-07-23|        72000|\n",
      "|  3|Charlie|        IT|  7000|   1992-03-15|        84000|\n",
      "|  4|  David|        IT|  6500|   1993-11-30|        78000|\n",
      "|  5|    Eva|        HR|  4800|   1991-09-17|        57600|\n",
      "+---+-------+----------+------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = df.withColumn(\"annual_salary\", col('salary')*12).drop(\"city\").withColumnRenamed('dob','date_of_birth')\n",
    "\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b94079e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+-------------+-------------+\n",
      "| id|   name|department|salary|date_of_birth|annual_salary|\n",
      "+---+-------+----------+------+-------------+-------------+\n",
      "|  1|  Alice|        HR|  5000|   1990-01-01|        60000|\n",
      "|  2|    Bob|   Finance|  6000|   1985-07-23|        72000|\n",
      "|  3|Charlie|        IT|  7000|   1992-03-15|        84000|\n",
      "|  4|  David|        IT|  6500|   1993-11-30|        78000|\n",
      "|  5|    Eva|        HR|  4800|   1991-09-17|        57600|\n",
      "+---+-------+----------+------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = df.selectExpr(\"id\", 'name', \"department\", 'salary', \"dob as date_of_birth\", \"salary * 12 as annual_salary\")\n",
    "\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a1fae83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+------+-------------+-------------+\n",
      "| id| name|department|salary|date_of_birth|annual_salary|\n",
      "+---+-----+----------+------+-------------+-------------+\n",
      "|  1|Alice|        HR|  5000|   1990-01-01|        60000|\n",
      "|  2|  Bob|   Finance|  6000|   1985-07-23|        72000|\n",
      "|  5|  Eva|        HR|  4800|   1991-09-17|        57600|\n",
      "+---+-----+----------+------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3 = df_2.filter(col(\"annual_salary\").between(50000, 75000))\n",
    "\n",
    "df_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4698a8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+----------+---------+\n",
      "| id|   name|department|salary|       dob|     city|\n",
      "+---+-------+----------+------+----------+---------+\n",
      "|  1|  Alice|        HR|  5000|1990-01-01|Bangalore|\n",
      "|  3|Charlie|        IT|  7000|1992-03-15|    Delhi|\n",
      "|  4|  David|        IT|  6500|1993-11-30|  Chennai|\n",
      "|  5|    Eva|        HR|  4800|1991-09-17|     Pune|\n",
      "+---+-------+----------+------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_4 = df.filter(col(\"department\").isin(\"IT\", \"HR\"))\n",
    "\n",
    "df_4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ca9837e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+------+----------+---------+\n",
      "| id| name|department|salary|       dob|     city|\n",
      "+---+-----+----------+------+----------+---------+\n",
      "|  1|Alice|        HR|  5000|1990-01-01|Bangalore|\n",
      "+---+-----+----------+------+----------+---------+\n",
      "\n",
      "+---+-------+----------+------+----------+---------+\n",
      "| id|   name|department|salary|       dob|     city|\n",
      "+---+-------+----------+------+----------+---------+\n",
      "|  1|  Alice|        HR|  5000|1990-01-01|Bangalore|\n",
      "|  3|Charlie|        IT|  7000|1992-03-15|    Delhi|\n",
      "+---+-------+----------+------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_like = df.filter(col(\"name\").like(\"A%\"))\n",
    "\n",
    "df_like.show()\n",
    "\n",
    "df_rgex = df.filter(col(\"name\").rlike(\"e$\"))\n",
    "\n",
    "df_rgex.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b06694ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+--------+\n",
      "| id| name|             hobbies|   hobby|\n",
      "+---+-----+--------------------+--------+\n",
      "|  1|Alice|[Reading, Cycling...| Reading|\n",
      "|  1|Alice|[Reading, Cycling...| Cycling|\n",
      "|  1|Alice|[Reading, Cycling...|Painting|\n",
      "|  2|  Bob|   [Gaming, Running]|  Gaming|\n",
      "|  2|  Bob|   [Gaming, Running]| Running|\n",
      "+---+-----+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", [\"Reading\", \"Cycling\", \"Painting\"]),\n",
    "    (2, \"Bob\", [\"Gaming\", \"Running\"]),\n",
    "    (3, \"Charlie\", [])\n",
    "]\n",
    "\n",
    "schema = [\"id\", \"name\", \"hobbies\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df_flat = df.withColumn(\"hobby\", explode(\"hobbies\"))\n",
    "\n",
    "df_flat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b2f26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+------+\n",
      "| id| name|     city|   zip|\n",
      "+---+-----+---------+------+\n",
      "|  1|Alice|Bangalore|560001|\n",
      "|  2|  Bob|    Delhi|110001|\n",
      "+---+-----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    Row(id=1, name=\"Alice\", address=Row(city=\"Bangalore\", zip=560001)),\n",
    "    Row(id=2, name=\"Bob\", address=Row(city=\"Delhi\", zip=110001))\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.select(\"id\", \"name\", \"address.city\", \"address.zip\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9724cdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|scores[math]|\n",
      "+---+------------+\n",
      "|  1|          90|\n",
      "|  2|          75|\n",
      "|  3|        NULL|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, {\"math\": 90, \"science\": 85}),\n",
    "    (2, {\"math\": 75, \"science\": 88}),\n",
    "    (3, {\"science\": 80})  # missing math\n",
    "]\n",
    "\n",
    "schema = [\"id\", \"scores\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.select(\"id\", col(\"scores\")[\"math\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6fd5bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [Row(id=1, scores={'science': 85, 'math': 90})],\n",
       " [],\n",
       " [],\n",
       " [Row(id=2, scores={'science': 88, 'math': 75})],\n",
       " [],\n",
       " [Row(id=3, scores={'science': 80})]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.glom().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c795671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------+-------------------+------------+--------------+-------+\n",
      "|               items|payment_mode|region|          timestamp|total_amount|transaction_id|user_id|\n",
      "+--------------------+------------+------+-------------------+------------+--------------+-------+\n",
      "|     [laptop, mouse]|        card| north|2025-08-25T10:30:00|      1500.0|         tx001|    u01|\n",
      "|             [phone]|         UPI| south|2025-08-24T18:20:00|       800.0|         tx002|    u02|\n",
      "|[headphones, char...|         COD| north|2025-08-26T09:00:00|       250.0|         tx003|    u03|\n",
      "+--------------------+------------+------+-------------------+------------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the JSON\n",
    "df = spark.read.option(\"multiline\", \"true\").json(r\"C:\\Users\\user\\Downloads\\txn.json\")\n",
    "\n",
    "# Sort globally by total_amount descending\n",
    "df_sorted = df.orderBy(col(\"total_amount\").desc())\n",
    "df_sorted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61469eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------+-------------------+------------+--------------+-------+\n",
      "|               items|payment_mode|region|          timestamp|total_amount|transaction_id|user_id|\n",
      "+--------------------+------------+------+-------------------+------------+--------------+-------+\n",
      "|             [phone]|         UPI| south|2025-08-24T18:20:00|       800.0|         tx002|    u02|\n",
      "|     [laptop, mouse]|        card| north|2025-08-25T10:30:00|      1500.0|         tx001|    u01|\n",
      "|[headphones, char...|         COD| north|2025-08-26T09:00:00|       250.0|         tx003|    u03|\n",
      "+--------------------+------------+------+-------------------+------------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by timestamp ascending\n",
    "df.sort(\"timestamp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bdc705b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------+-------------------+------------+--------------+-------+\n",
      "|               items|payment_mode|region|          timestamp|total_amount|transaction_id|user_id|\n",
      "+--------------------+------------+------+-------------------+------------+--------------+-------+\n",
      "|[headphones, char...|         COD| north|2025-08-26T09:00:00|       250.0|         tx003|    u03|\n",
      "|     [laptop, mouse]|        card| north|2025-08-25T10:30:00|      1500.0|         tx001|    u01|\n",
      "|             [phone]|         UPI| south|2025-08-24T18:20:00|       800.0|         tx002|    u02|\n",
      "+--------------------+------------+------+-------------------+------------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Partition by region and sort within partitions by total_amount ascending\n",
    "df_partitioned = df.repartition(8, \"region\")\n",
    "df_partitioned.sortWithinPartitions(\"total_amount\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f099ac19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
